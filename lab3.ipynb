{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Mining: Lab3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of DEC\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating accuracy of unsupervised clustering\n",
    "Problem is that we don't know what label's names clustering algorithm would provide as result. So we must match all possible permutations of label's names assigments\n",
    "In fact we can use Hungarian algorithm (Munkres) for not brute-force all possible permutations and choose the best suitable for **$O(n^3)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use implemented munkres algorithm\n",
    "from munkres import Munkres\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def accuracy(true_labels, labels):\n",
    "    \n",
    "    n = np.unique(labels).size\n",
    "    # Matrix[i][j] for calculate amount of true predictions for label `j` as label `i`\n",
    "    matrix = np.zeros((n, n))\n",
    "\n",
    "    for true, predict in zip(true_labels, labels):\n",
    "        for i in range(n):\n",
    "            if true == i:\n",
    "                matrix[predict][i] += 1\n",
    "    cost_matrix = matrix * (-1)  # Due to munkres algo minimize cost\n",
    "    m = Munkres()\n",
    "    indexes = m.compute(cost_matrix)  # Best case assignments of labels\n",
    "\n",
    "    n_samples = labels.size\n",
    "    n_true_predicted = sum([matrix[ind[0]][ind[1]] for ind in indexes])\n",
    "\n",
    "    return n_true_predicted / n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "n_samples = 10000\n",
    "\n",
    "X = x_train[:n_samples]\n",
    "Y = y_train[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Mnist data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert square image representation to flat arrays (28*28 -> 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    flat_X = []\n",
    "    for sample in X:\n",
    "        flat_X.append(np.array(sample).flatten())\n",
    "\n",
    "    return np.array(flat_X)\n",
    "\n",
    "X = flatten(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Clustering MNIST data with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, tol=0.001, n_init=20)\n",
    "\n",
    "def kmeans_fit(X):\n",
    "    kmeans.fit(X)\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5324"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = kmeans_fit(X)\n",
    "accuracy(Y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is about **53%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Clustering MNIST data with AE + K-means (use the best trained AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\DATA\\Projects\\Bach 3\\Data Mining\\HW1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nekit\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7531"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess X data\n",
    "X_p = X / 255.\n",
    "X_p = np.reshape(X_p, (len(X_p), 28, 28, 1))\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load Encoder (Ensure correctness of file placement)\n",
    "# Comand below can help you with directory changing\n",
    "# %cd \"C:\\DATA\\Projects\\Bach 3\\Data Mining\\HW1\"\n",
    "\n",
    "encoder = load_model(\"encoder_model_saved.h5\")\n",
    "encoded = encoder.predict(X_p, batch_size=256)\n",
    "\n",
    "y_pred = kmeans_fit(encoded)\n",
    "accuracy(Y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is about **75%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Clustering MNIST data with DEC\n",
    "\n",
    "Create Keras Layer for clustering with student t-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    # Example\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Target Distribution  (P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cluster centers from AE + K-Means start train DEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations 140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8117"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.engine.topology import InputSpec\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Below we use relults of AE + K-Means clusterization \n",
    "\n",
    "clustering_layer = ClusteringLayer(10, name='clustering', weights=[kmeans.cluster_centers_])(encoder.output)\n",
    "dec_model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "dec_model.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "\n",
    "maxiter = 20000\n",
    "batch_size = 256\n",
    "tol = 0.001\n",
    "update_interval = 140\n",
    "loss = 0\n",
    "\n",
    "y_pred_last = np.copy(y_pred) \n",
    "index = 0\n",
    "index_array = np.arange(X_p.shape[0])\n",
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = dec_model.predict(X_p, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            break\n",
    "\n",
    "    idx = index_array[index * batch_size: min((index + 1) * batch_size, X_p.shape[0])]\n",
    "    loss = dec_model.train_on_batch(x=X_p[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= X_p.shape[0] else 0\n",
    "print(\"Number of iterations\", ite)\n",
    "\n",
    "accuracy(Y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
